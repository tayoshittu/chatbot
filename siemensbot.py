# This is a chatbot using siemens wiki page and website as the corpus
# the website was scraped and the contents were stored in a text file
# the contents of the textfile was cleaned and preprocessed

# import libraries
import io
import random
import string # to process standard python strings
import warnings
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

from flask import Flask, render_template, request

import nltk
from nltk.stem import WordNetLemmatizer

app = Flask(__name__)

# Download packages
nltk.download('popular', quiet=True)
nltk.download('punkt') 
nltk.download('wordnet') 


# Reading in the corpus, siemens_content.txt
with open('siemens_content.txt','r', encoding='utf8', errors ='ignore') as fin:
    raw = fin.read().lower()

# converts to list of sentences and words
sentence_tokens = nltk.sent_tokenize(raw) 
word_tokens = nltk.word_tokenize(raw)

# Preprocessing: Normalize Tokens
lemma = WordNetLemmatizer()
def LeTokens(tokens):
    """
    Apply lemmatization to a list of tokens.

    Args:
        tokens (list): List of tokens.

    Returns:
        list: Lemmatized tokens.
    """
    return [lemma.lemmatize(token) for token in tokens]

# Create a dictionary to remove punctuation characters
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)

def LeNormalize(text):
    """
    Normalize the given text by performing tokenization, lowercasing, punctuation removal, and lemmatization.

    Args:
        text (str): Input text.

    Returns:
        list: Normalized tokens.
    """
    return LeTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

# Route for root url ("/") in flask

@app.route("/")
def main():
    # Render HTML templates
    return render_template("index.html")




INTRO_INPUTS = ("hello", "hi", "good morning", "wassup", "what's up","hey",)

INTRO_RESPONSES = [
    "hi",
    "hey",
    "Finally! someone speaks to me",
    "hi there",
    "hello",
    "Hello, I'm Lindsay Your Virtual Assistant"
]

# Creates a standard introductory messages
def intro(sentence):
    
    """
    Checks if the given sentence contains an introductory phrase.

    Parameters:
    - sentence: A string representing the user's input sentence.

    Returns:
    - A randomly selected introductory response from INTRO_RESPONSES if an introductory phrase is found,
      None otherwise.
    """
    
    for word in sentence.split():
        if word.lower() in INTRO_INPUTS:
            return random.choice(INTRO_RESPONSES)


# Generating bot response using document similarity
def get_response(user_response):

    # Initialize the response variable
    lindsay_response = ''  

    # Add user response to the list of sentence tokens
    sentence_tokens.append(user_response)

    # Create a TfidfVectorizer with the LeNormalize tokenizer and English stop words
    TfidfVec = TfidfVectorizer(tokenizer=LeNormalize, stop_words='english')

    # Transform the sentence tokens into TF-IDF vectors
    tfidf = TfidfVec.fit_transform(sentence_tokens)

    # Compute the cosine similarity between the last user response vector and all other vectors
    vals = cosine_similarity(tfidf[-1], tfidf)

    # Get the index of the most similar vector
    idx = vals.argsort()[0][-2]

    # Flatten and sort the cosine similarity values
    flat = vals.flatten()
    flat.sort()

    # Get the second highest cosine similarity value
    req_tfidf = flat[-2]

    if req_tfidf == 0:
        # If the similarity is zero, respond with a default message
        lindsay_response = lindsay_response + "I am sorry! I don't understand you"
        return lindsay_response
    else:
        # If there is a similarity, retrieve the corresponding sentence token
        lindsay_response = lindsay_response + sentence_tokens[idx]
        return lindsay_response


@app.route("/get")
def get_siemensbot_response():

    """
    Handles the GET request to '/get' route and returns the response from Siemens Bot.

    Returns:
    - The response generated by Siemens Bot based on the user's input.
    """

    # Retrieves user input using request
    userText = request.args.get('userMessage')
    # Converts to lowercase
    userText = userText.lower()
    # if the users input is not bye
    if userText != 'bye':
        # if the input is "thanks" or "thank you"
        if userText == 'thanks' or userText == 'thank you':
            # Siemens Bot returns "it was my pleasure"
            return "It was my pleasure"
        else:
            # if input matches intro it returns corresponding intro response
            if intro(userText) is not None:
                return intro(userText)
            else:
                # else generate response
                return get_response(userText)
    else:
        # if bye, respond with "have a great day, bye"
        return "Have a great day, Bye."
    
        

if __name__ == '__main__':
    app.run(debug=True)